{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "\n",
    "In this notebook, we demonstrate a simple form of transfer learning, which uses pre-trained neural networks that are already trained on other (similar) datasets/tasks. There are two ways to use them:\n",
    "  * as initializers (warm start)\n",
    "  * as fixed feature extractors\n",
    "\n",
    "We will also introduce the `tensorflow.keras.applications` interface which includes many pre-trained models that are hugely successful for their respective application domains. For actual practical use, you should start, whenever possible, with some of these models as baselines to modify and build your custom architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tqdm.keras import TqdmCallback\n",
    "sns.set(font_scale=1.5, style='dark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the familiar [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset we used previously.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/944/1*6XQqOifwnmplS22zCRRVaw.png \"CIFAR10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Canned Models\n",
    "\n",
    "Whenever possible, we should start with some canned models already developed by careful testing and fine-tuning. The `tensorflow.keras.applications` module is a collection of such models trained on enormous datasets such as the [ImageNet](http://www.image-net.org/) dataset.\n",
    "\n",
    "\n",
    "Here, we will use the [ResNet](https://arxiv.org/abs/1512.03385) architecture which is immensely successful at image recognition and related tasks.\n",
    "![alt text](https://miro.medium.com/max/1524/1*6hF97Upuqg_LdsqWY6n_wg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet import ResNet50\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will randomly set the weights of the ResNet by setting `weights=None`. The include_top option says that we will keep the classification layers of the ResNet, since we are not using pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = ResNet50(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_shape=(32, 32, 3),\n",
    "    classes=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baseline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save(model, path, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper around the fit method to save\n",
    "    results and load if saved files are found\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    path.mkdir(exist_ok=True)\n",
    "    model_path = path.joinpath('model.h5')\n",
    "    history_path = path.joinpath('history.json')\n",
    "    if model_path.exists() and history_path.exists():\n",
    "        model.load_weights(str(model_path))\n",
    "        history = pd.read_json(str(history_path))\n",
    "    else:\n",
    "        model.compile(\n",
    "            loss='categorical_crossentropy', optimizer=Adam(0.0001), metrics=['accuracy'])\n",
    "        history = model.fit(**kwargs)\n",
    "        history = pd.DataFrame(history.history)\n",
    "        history.to_json(str(history_path))\n",
    "        model.save_weights(str(model_path))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline, baseline_history = train_and_save(\n",
    "    model=baseline,\n",
    "    path='resnet_cifar10_from_scratch',\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    verbose=0,\n",
    "    callbacks=[TqdmCallback(verbose=1)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_history.plot(x=None, y=['accuracy', 'val_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Canned Models with Pre-trained Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use ResNet with pre-trained weights, which are obtained on training on the ImageNet dataset. This can be done by simply setting `weights='imagenet'`.\n",
    "\n",
    "\n",
    "This has 1000 classes, which is not what we need here. Hence, we will set `include_top=False` to set our own classification layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50(\n",
    "    include_top=False,  # Do not include top\n",
    "    weights='imagenet',  # load imagenet weights\n",
    "    input_shape=(32, 32, 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model here doesn't have a classification layer, we will build one accommodating our 10 class problem here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = Sequential(layers=[base_model])\n",
    "pretrained_model.add(GlobalAveragePooling2D())\n",
    "pretrained_model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model, pretrained_history = train_and_save(\n",
    "    model=pretrained_model,\n",
    "    path='resnet_cifar10_pretrained',\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    verbose=0,\n",
    "    callbacks=[TqdmCallback(verbose=1)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_history.plot(x=None, y=['accuracy', 'val_accuracy'])\n",
    "baseline_history.plot(x=None, y=['accuracy', 'val_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that we obtain a much faster performance using pre-trained weights to warm start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Canned Models with Fixed Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we allowed all the weights in the combined network to vary during training. Hence, the ImageNet weights were only used as a warm start tool.\n",
    "\n",
    "Here, we will explore an alternative, where the `base_model`'s weights are held constant and not trained. This is easily done by supplying the flag\n",
    "```python\n",
    "    model.trainable = False\n",
    "```\n",
    "This can also be set on layers\n",
    "```python\n",
    "    layer.trainable = False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50(\n",
    "    include_top=False,  # Do not include top\n",
    "    weights='imagenet',  # load imagenet weights\n",
    "    input_shape=(32, 32, 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have set here\n",
    "```python\n",
    "    base_model.trainable = False\n",
    "```\n",
    "However, this is not good if we have batch normalization layers. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in base_model.layers:\n",
    "    if '_bn' not in l.name:\n",
    "        l.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification layers will not be held constant and shall be the only layers that are trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_v2 = Sequential(layers=[base_model])\n",
    "pretrained_model_v2.add(GlobalAveragePooling2D())\n",
    "pretrained_model_v2.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_v2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_v2, pretrained_v2_history = train_and_save(\n",
    "    model=pretrained_model_v2,\n",
    "    path='resnet_cifar10_pretrained_v2',\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    verbose=0,\n",
    "    callbacks=[TqdmCallback(verbose=1)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_v2_history.plot(x=None, y=['accuracy', 'val_accuracy'])\n",
    "baseline_history.plot(x=None, y=['accuracy', 'val_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, by fixing all the base_model weights we can also obtain better results than the baseline. In fact, observe that the generalization gap is much better, because most of the weights have been fixed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "1. Train the above networks to completion using various other regularization techniques to get the best possible performance. Compare with our earlier investigations on CIFAR10.\n",
    "2. Try transfer learning techniques on other types of data, e.g. RNN-type on language applications. There are much fewer pre-trained models in this direction, however, so you may have to implement your own pre-trained models to learn across tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "library.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
